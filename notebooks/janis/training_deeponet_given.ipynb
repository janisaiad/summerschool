{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d6faae",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 17:39:35.126856: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-04 17:39:35.152268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754343575.168595   54255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754343575.174596   54255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754343575.195221   54255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754343575.195245   54255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754343575.195246   54255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754343575.195248   54255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-04 17:39:35.200335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sciml.model.deeponet import DeepONet\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sciml.data.preprocessing.process_given_dataset import get_mu_xs_sol\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ddf854",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "d_p = 40\n",
    "d_V = 40\n",
    "epochs = 300    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c0fd49",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754343578.387731   54255 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "internal_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(100,)),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "])\n",
    "\n",
    "\n",
    "external_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(1,)),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9a396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/janis/SCIML/summerschool/data/benchmarks/given/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261ac835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 17:39:39,265 - sciml.model.deeponet.deeponet - INFO - Model initialized with 300 epochs, 32 batch size, 0.001 learning rate\n"
     ]
    }
   ],
   "source": [
    "model = DeepONet(regular_params={\"internal_model\": internal_model, \"external_model\": external_model}, hyper_params={\"d_p\": d_p, \"d_V\": d_V,\"device\": \"GPU\",\"n_epochs\":epochs},folder_path=\"../../data/benchmarks/given/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8385d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mus.shape (2000, 100)\n",
      "xs.shape (2000, 100, 1)\n",
      "sol.shape (2000, 100)\n"
     ]
    }
   ],
   "source": [
    "mus, xs, sol = get_mu_xs_sol(folder_path,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b511c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 100)\n",
      "(2000, 100, 1)\n",
      "(2000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(mus.shape)\n",
    "print(xs.shape)\n",
    "print(sol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84bff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mus.shape (2000, 100)\n",
      "xs.shape (2000, 100, 1)\n",
      "sol.shape (2000, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 17:39:39.749972: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Training progress:   0%|          | 0/300 [00:00<?, ?it/s]2025-08-04 17:39:43.288425: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-04 17:39:43,475 - sciml.model.deeponet.deeponet - INFO - Epoch 1/300\n",
      "2025-08-04 17:39:43,477 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.076964\n",
      "2025-08-04 17:39:43,479 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.064207\n",
      "Training progress:   0%|          | 1/300 [00:03<18:19,  3.68s/it]2025-08-04 17:39:45.729777: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-04 17:39:45,845 - sciml.model.deeponet.deeponet - INFO - Epoch 2/300\n",
      "2025-08-04 17:39:45,846 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.063990\n",
      "2025-08-04 17:39:45,847 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.062381\n",
      "Training progress:   1%|          | 2/300 [00:06<14:26,  2.91s/it]2025-08-04 17:39:48,158 - sciml.model.deeponet.deeponet - INFO - Epoch 3/300\n",
      "2025-08-04 17:39:48,159 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.060489\n",
      "2025-08-04 17:39:48,160 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.057478\n",
      "Training progress:   1%|          | 3/300 [00:08<13:02,  2.64s/it]2025-08-04 17:39:50.709323: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-04 17:39:50,861 - sciml.model.deeponet.deeponet - INFO - Epoch 4/300\n",
      "2025-08-04 17:39:50,863 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.055663\n",
      "2025-08-04 17:39:50,864 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.055344\n",
      "Training progress:   1%|▏         | 4/300 [00:11<13:08,  2.66s/it]2025-08-04 17:39:53,335 - sciml.model.deeponet.deeponet - INFO - Epoch 5/300\n",
      "2025-08-04 17:39:53,337 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.054206\n",
      "2025-08-04 17:39:53,338 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.055476\n",
      "Training progress:   2%|▏         | 5/300 [00:13<12:45,  2.59s/it]2025-08-04 17:39:55,893 - sciml.model.deeponet.deeponet - INFO - Epoch 6/300\n",
      "2025-08-04 17:39:55,896 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.053509\n",
      "2025-08-04 17:39:55,897 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.053360\n",
      "Training progress:   2%|▏         | 6/300 [00:16<12:39,  2.58s/it]2025-08-04 17:39:58,315 - sciml.model.deeponet.deeponet - INFO - Epoch 7/300\n",
      "2025-08-04 17:39:58,317 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.051924\n",
      "2025-08-04 17:39:58,317 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.052265\n",
      "Training progress:   2%|▏         | 7/300 [00:18<12:21,  2.53s/it]2025-08-04 17:40:00.454149: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-04 17:40:00,582 - sciml.model.deeponet.deeponet - INFO - Epoch 8/300\n",
      "2025-08-04 17:40:00,583 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.051304\n",
      "2025-08-04 17:40:00,583 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.051745\n",
      "Training progress:   3%|▎         | 8/300 [00:20<11:54,  2.45s/it]2025-08-04 17:40:02,957 - sciml.model.deeponet.deeponet - INFO - Epoch 9/300\n",
      "2025-08-04 17:40:02,958 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.051016\n",
      "2025-08-04 17:40:02,959 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.051664\n",
      "Training progress:   3%|▎         | 9/300 [00:23<11:45,  2.42s/it]2025-08-04 17:40:05,232 - sciml.model.deeponet.deeponet - INFO - Epoch 10/300\n",
      "2025-08-04 17:40:05,233 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.050872\n",
      "2025-08-04 17:40:05,233 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.051500\n",
      "Training progress:   3%|▎         | 10/300 [00:25<11:29,  2.38s/it]2025-08-04 17:40:07,492 - sciml.model.deeponet.deeponet - INFO - Epoch 11/300\n",
      "2025-08-04 17:40:07,493 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.050701\n",
      "2025-08-04 17:40:07,494 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.051377\n",
      "Training progress:   4%|▎         | 11/300 [00:27<11:16,  2.34s/it]2025-08-04 17:40:09,710 - sciml.model.deeponet.deeponet - INFO - Epoch 12/300\n",
      "2025-08-04 17:40:09,711 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.050614\n",
      "2025-08-04 17:40:09,712 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.051156\n",
      "Training progress:   4%|▍         | 12/300 [00:29<11:03,  2.30s/it]2025-08-04 17:40:11,853 - sciml.model.deeponet.deeponet - INFO - Epoch 13/300\n",
      "2025-08-04 17:40:11,854 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.050445\n",
      "2025-08-04 17:40:11,855 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050972\n",
      "Training progress:   4%|▍         | 13/300 [00:32<10:47,  2.26s/it]2025-08-04 17:40:13,966 - sciml.model.deeponet.deeponet - INFO - Epoch 14/300\n",
      "2025-08-04 17:40:13,967 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.050276\n",
      "2025-08-04 17:40:13,968 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050843\n",
      "Training progress:   5%|▍         | 14/300 [00:34<10:32,  2.21s/it]2025-08-04 17:40:16,086 - sciml.model.deeponet.deeponet - INFO - Epoch 15/300\n",
      "2025-08-04 17:40:16,087 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.050542\n",
      "2025-08-04 17:40:16,088 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050743\n",
      "Training progress:   5%|▌         | 15/300 [00:36<10:22,  2.18s/it]2025-08-04 17:40:18.041658: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-04 17:40:18,154 - sciml.model.deeponet.deeponet - INFO - Epoch 16/300\n",
      "2025-08-04 17:40:18,154 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049983\n",
      "2025-08-04 17:40:18,155 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050518\n",
      "Training progress:   5%|▌         | 16/300 [00:38<10:10,  2.15s/it]2025-08-04 17:40:20,240 - sciml.model.deeponet.deeponet - INFO - Epoch 17/300\n",
      "2025-08-04 17:40:20,241 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049800\n",
      "2025-08-04 17:40:20,241 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050399\n",
      "Training progress:   6%|▌         | 17/300 [00:40<10:02,  2.13s/it]2025-08-04 17:40:22,398 - sciml.model.deeponet.deeponet - INFO - Epoch 18/300\n",
      "2025-08-04 17:40:22,399 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049727\n",
      "2025-08-04 17:40:22,400 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050336\n",
      "Training progress:   6%|▌         | 18/300 [00:42<10:03,  2.14s/it]2025-08-04 17:40:24,609 - sciml.model.deeponet.deeponet - INFO - Epoch 19/300\n",
      "2025-08-04 17:40:24,610 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049779\n",
      "2025-08-04 17:40:24,612 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050173\n",
      "Training progress:   6%|▋         | 19/300 [00:44<10:07,  2.16s/it]2025-08-04 17:40:26,825 - sciml.model.deeponet.deeponet - INFO - Epoch 20/300\n",
      "2025-08-04 17:40:26,826 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049526\n",
      "2025-08-04 17:40:26,827 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050116\n",
      "Training progress:   7%|▋         | 20/300 [00:47<10:09,  2.18s/it]2025-08-04 17:40:29,287 - sciml.model.deeponet.deeponet - INFO - Epoch 21/300\n",
      "2025-08-04 17:40:29,290 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049515\n",
      "2025-08-04 17:40:29,291 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050090\n",
      "Training progress:   7%|▋         | 21/300 [00:49<10:31,  2.26s/it]2025-08-04 17:40:32,055 - sciml.model.deeponet.deeponet - INFO - Epoch 22/300\n",
      "2025-08-04 17:40:32,059 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049611\n",
      "2025-08-04 17:40:32,060 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050093\n",
      "Training progress:   7%|▋         | 22/300 [00:52<11:11,  2.41s/it]2025-08-04 17:40:34,122 - sciml.model.deeponet.deeponet - INFO - Epoch 23/300\n",
      "2025-08-04 17:40:34,123 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049471\n",
      "2025-08-04 17:40:34,124 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050085\n",
      "Training progress:   8%|▊         | 23/300 [00:54<10:39,  2.31s/it]2025-08-04 17:40:36,201 - sciml.model.deeponet.deeponet - INFO - Epoch 24/300\n",
      "2025-08-04 17:40:36,202 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049471\n",
      "2025-08-04 17:40:36,202 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050111\n",
      "Training progress:   8%|▊         | 24/300 [00:56<10:18,  2.24s/it]2025-08-04 17:40:38,244 - sciml.model.deeponet.deeponet - INFO - Epoch 25/300\n",
      "2025-08-04 17:40:38,245 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049393\n",
      "2025-08-04 17:40:38,245 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049981\n",
      "Training progress:   8%|▊         | 25/300 [00:58<09:59,  2.18s/it]2025-08-04 17:40:40,364 - sciml.model.deeponet.deeponet - INFO - Epoch 26/300\n",
      "2025-08-04 17:40:40,365 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049348\n",
      "2025-08-04 17:40:40,366 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049941\n",
      "Training progress:   9%|▊         | 26/300 [01:00<09:52,  2.16s/it]2025-08-04 17:40:42,503 - sciml.model.deeponet.deeponet - INFO - Epoch 27/300\n",
      "2025-08-04 17:40:42,503 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049341\n",
      "2025-08-04 17:40:42,504 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.050041\n",
      "Training progress:   9%|▉         | 27/300 [01:02<09:48,  2.16s/it]2025-08-04 17:40:44,683 - sciml.model.deeponet.deeponet - INFO - Epoch 28/300\n",
      "2025-08-04 17:40:44,684 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049448\n",
      "2025-08-04 17:40:44,685 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049913\n",
      "Training progress:   9%|▉         | 28/300 [01:04<09:48,  2.16s/it]2025-08-04 17:40:46,685 - sciml.model.deeponet.deeponet - INFO - Epoch 29/300\n",
      "2025-08-04 17:40:46,685 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049255\n",
      "2025-08-04 17:40:46,686 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049850\n",
      "Training progress:  10%|▉         | 29/300 [01:06<09:33,  2.11s/it]2025-08-04 17:40:48,730 - sciml.model.deeponet.deeponet - INFO - Epoch 30/300\n",
      "2025-08-04 17:40:48,731 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049233\n",
      "2025-08-04 17:40:48,732 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049831\n",
      "Training progress:  10%|█         | 30/300 [01:08<09:25,  2.09s/it]2025-08-04 17:40:50,775 - sciml.model.deeponet.deeponet - INFO - Epoch 31/300\n",
      "2025-08-04 17:40:50,776 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049205\n",
      "2025-08-04 17:40:50,776 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049853\n",
      "Training progress:  10%|█         | 31/300 [01:10<09:19,  2.08s/it]2025-08-04 17:40:52.734768: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-04 17:40:52,847 - sciml.model.deeponet.deeponet - INFO - Epoch 32/300\n",
      "2025-08-04 17:40:52,848 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049357\n",
      "2025-08-04 17:40:52,849 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049839\n",
      "Training progress:  11%|█         | 32/300 [01:13<09:16,  2.08s/it]2025-08-04 17:40:54,968 - sciml.model.deeponet.deeponet - INFO - Epoch 33/300\n",
      "2025-08-04 17:40:54,969 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049198\n",
      "2025-08-04 17:40:54,970 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049750\n",
      "Training progress:  11%|█         | 33/300 [01:15<09:18,  2.09s/it]2025-08-04 17:40:57,117 - sciml.model.deeponet.deeponet - INFO - Epoch 34/300\n",
      "2025-08-04 17:40:57,117 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049186\n",
      "2025-08-04 17:40:57,118 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049715\n",
      "Training progress:  11%|█▏        | 34/300 [01:17<09:20,  2.11s/it]2025-08-04 17:40:59,226 - sciml.model.deeponet.deeponet - INFO - Epoch 35/300\n",
      "2025-08-04 17:40:59,227 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049140\n",
      "2025-08-04 17:40:59,227 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049690\n",
      "Training progress:  12%|█▏        | 35/300 [01:19<09:18,  2.11s/it]2025-08-04 17:41:01,299 - sciml.model.deeponet.deeponet - INFO - Epoch 36/300\n",
      "2025-08-04 17:41:01,300 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049141\n",
      "2025-08-04 17:41:01,300 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049690\n",
      "Training progress:  12%|█▏        | 36/300 [01:21<09:13,  2.10s/it]2025-08-04 17:41:03,413 - sciml.model.deeponet.deeponet - INFO - Epoch 37/300\n",
      "2025-08-04 17:41:03,414 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049111\n",
      "2025-08-04 17:41:03,414 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049677\n",
      "Training progress:  12%|█▏        | 37/300 [01:23<09:12,  2.10s/it]2025-08-04 17:41:05,473 - sciml.model.deeponet.deeponet - INFO - Epoch 38/300\n",
      "2025-08-04 17:41:05,474 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049409\n",
      "2025-08-04 17:41:05,475 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049987\n",
      "Training progress:  13%|█▎        | 38/300 [01:25<09:07,  2.09s/it]2025-08-04 17:41:07,582 - sciml.model.deeponet.deeponet - INFO - Epoch 39/300\n",
      "2025-08-04 17:41:07,583 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049149\n",
      "2025-08-04 17:41:07,583 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049681\n",
      "Training progress:  13%|█▎        | 39/300 [01:27<09:06,  2.10s/it]2025-08-04 17:41:09,621 - sciml.model.deeponet.deeponet - INFO - Epoch 40/300\n",
      "2025-08-04 17:41:09,622 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049059\n",
      "2025-08-04 17:41:09,622 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049636\n",
      "Training progress:  13%|█▎        | 40/300 [01:29<09:00,  2.08s/it]2025-08-04 17:41:11,750 - sciml.model.deeponet.deeponet - INFO - Epoch 41/300\n",
      "2025-08-04 17:41:11,751 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049078\n",
      "2025-08-04 17:41:11,751 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049662\n",
      "Training progress:  14%|█▎        | 41/300 [01:31<09:02,  2.09s/it]2025-08-04 17:41:13,794 - sciml.model.deeponet.deeponet - INFO - Epoch 42/300\n",
      "2025-08-04 17:41:13,794 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049058\n",
      "2025-08-04 17:41:13,795 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049650\n",
      "Training progress:  14%|█▍        | 42/300 [01:33<08:56,  2.08s/it]2025-08-04 17:41:15,806 - sciml.model.deeponet.deeponet - INFO - Epoch 43/300\n",
      "2025-08-04 17:41:15,807 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049086\n",
      "2025-08-04 17:41:15,807 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049640\n",
      "Training progress:  14%|█▍        | 43/300 [01:36<08:49,  2.06s/it]2025-08-04 17:41:17,911 - sciml.model.deeponet.deeponet - INFO - Epoch 44/300\n",
      "2025-08-04 17:41:17,912 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049045\n",
      "2025-08-04 17:41:17,913 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049630\n",
      "Training progress:  15%|█▍        | 44/300 [01:38<08:50,  2.07s/it]2025-08-04 17:41:19,957 - sciml.model.deeponet.deeponet - INFO - Epoch 45/300\n",
      "2025-08-04 17:41:19,958 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049069\n",
      "2025-08-04 17:41:19,959 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049618\n",
      "Training progress:  15%|█▌        | 45/300 [01:40<08:46,  2.07s/it]2025-08-04 17:41:21,996 - sciml.model.deeponet.deeponet - INFO - Epoch 46/300\n",
      "2025-08-04 17:41:21,996 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049045\n",
      "2025-08-04 17:41:21,997 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049604\n",
      "Training progress:  15%|█▌        | 46/300 [01:42<08:42,  2.06s/it]2025-08-04 17:41:24,063 - sciml.model.deeponet.deeponet - INFO - Epoch 47/300\n",
      "2025-08-04 17:41:24,064 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049052\n",
      "2025-08-04 17:41:24,065 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049598\n",
      "Training progress:  16%|█▌        | 47/300 [01:44<08:41,  2.06s/it]2025-08-04 17:41:26,188 - sciml.model.deeponet.deeponet - INFO - Epoch 48/300\n",
      "2025-08-04 17:41:26,189 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049036\n",
      "2025-08-04 17:41:26,190 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049587\n",
      "Training progress:  16%|█▌        | 48/300 [01:46<08:44,  2.08s/it]2025-08-04 17:41:28,349 - sciml.model.deeponet.deeponet - INFO - Epoch 49/300\n",
      "2025-08-04 17:41:28,349 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049026\n",
      "2025-08-04 17:41:28,350 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049568\n",
      "Training progress:  16%|█▋        | 49/300 [01:48<08:48,  2.10s/it]2025-08-04 17:41:30,512 - sciml.model.deeponet.deeponet - INFO - Epoch 50/300\n",
      "2025-08-04 17:41:30,513 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049023\n",
      "2025-08-04 17:41:30,513 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049559\n",
      "Training progress:  17%|█▋        | 50/300 [01:50<08:50,  2.12s/it]2025-08-04 17:41:32,647 - sciml.model.deeponet.deeponet - INFO - Epoch 51/300\n",
      "2025-08-04 17:41:32,647 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049153\n",
      "2025-08-04 17:41:32,648 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049829\n",
      "Training progress:  17%|█▋        | 51/300 [01:52<08:49,  2.13s/it]2025-08-04 17:41:34,935 - sciml.model.deeponet.deeponet - INFO - Epoch 52/300\n",
      "2025-08-04 17:41:34,936 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049112\n",
      "2025-08-04 17:41:34,937 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049563\n",
      "Training progress:  17%|█▋        | 52/300 [01:55<08:59,  2.17s/it]2025-08-04 17:41:37,089 - sciml.model.deeponet.deeponet - INFO - Epoch 53/300\n",
      "2025-08-04 17:41:37,090 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049004\n",
      "2025-08-04 17:41:37,090 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049556\n",
      "Training progress:  18%|█▊        | 53/300 [01:57<08:55,  2.17s/it]2025-08-04 17:41:39,294 - sciml.model.deeponet.deeponet - INFO - Epoch 54/300\n",
      "2025-08-04 17:41:39,295 - sciml.model.deeponet.deeponet - INFO - Training Loss: 0.049025\n",
      "2025-08-04 17:41:39,295 - sciml.model.deeponet.deeponet - INFO - Test Loss: 0.049565\n",
      "Training progress:  18%|█▊        | 54/300 [02:00<09:08,  2.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_54255/2577976232.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_history = model.fit()\n",
      "\u001b[32m~/SCIML/summerschool/model/deeponet/deeponet.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, device, inputs, sol, given, type, training)\u001b[39m\n\u001b[32m    222\u001b[39m \n\u001b[32m    223\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;28;01min\u001b[39;00m tqdm(range(self.n_epochs),desc=\u001b[33m\"Training progress\"\u001b[39m):\n\u001b[32m    224\u001b[39m                 mean_loss = \u001b[32m0\u001b[39m\n\u001b[32m    225\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;28;01min\u001b[39;00m train_dataset:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m                     loss = self.train_step(batch)\n\u001b[32m    227\u001b[39m                     mean_loss += loss\n\u001b[32m    228\u001b[39m                 loss_history_train.append(float(mean_loss/len(train_dataset)))\n\u001b[32m    229\u001b[39m \n",
      "\u001b[32m~/SCIML/summerschool/model/deeponet/deeponet.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    266\u001b[39m             loss = self.loss_function(y_pred, sol)\n\u001b[32m    267\u001b[39m \n\u001b[32m    268\u001b[39m         \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[32m    269\u001b[39m         gradients = tape.gradient(loss, self.trainable_variables)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m         self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\u001b[32m    271\u001b[39m \n\u001b[32m    272\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply_gradients(self, grads_and_vars):\n\u001b[32m    462\u001b[39m         grads, trainable_variables = zip(*grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m         self.apply(grads, trainable_variables)\n\u001b[32m    464\u001b[39m         \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[32m    465\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._iterations\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    523\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    524\u001b[39m                     grads = [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g / scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;28;01min\u001b[39;00m grads]\n\u001b[32m    525\u001b[39m \n\u001b[32m    526\u001b[39m                 \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m                 self._backend_apply_gradients(grads, trainable_variables)\n\u001b[32m    528\u001b[39m                 \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[32m    529\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;28;01min\u001b[39;00m trainable_variables:\n\u001b[32m    530\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m variable.constraint \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables)\u001b[39m\n\u001b[32m    589\u001b[39m             grads = self._clip_gradients(grads)\n\u001b[32m    590\u001b[39m             self._apply_weight_decay(trainable_variables)\n\u001b[32m    591\u001b[39m \n\u001b[32m    592\u001b[39m             \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m             self._backend_update_step(\n\u001b[32m    594\u001b[39m                 grads, trainable_variables, self.learning_rate\n\u001b[32m    595\u001b[39m             )\n\u001b[32m    596\u001b[39m \n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, grads, trainable_variables, learning_rate)\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m trainable_variables\n\u001b[32m    117\u001b[39m         ]\n\u001b[32m    118\u001b[39m         grads_and_vars = list(zip(grads, trainable_variables))\n\u001b[32m    119\u001b[39m         grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[32m    121\u001b[39m             self._distributed_tf_update_step,\n\u001b[32m    122\u001b[39m             self._distribution_strategy,\n\u001b[32m    123\u001b[39m             grads_and_vars,\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fn, strategy, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m   Returns:\n\u001b[32m     48\u001b[39m     The \u001b[38;5;28;01mreturn\u001b[39;00m value of the `fn` call.\n\u001b[32m     49\u001b[39m   \"\"\"\n\u001b[32m     50\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, *args, **kwargs)\n\u001b[32m     52\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     return distribute_lib.get_replica_context().merge_call(\n\u001b[32m     54\u001b[39m         fn, args=args, kwargs=kwargs)\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, distribution, grads_and_vars, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n\u001b[32m    132\u001b[39m \n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;28;01min\u001b[39;00m grads_and_vars:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m             distribution.extended.update(\n\u001b[32m    135\u001b[39m                 var,\n\u001b[32m    136\u001b[39m                 apply_grad_to_update_var,\n\u001b[32m    137\u001b[39m                 args=(grad, learning_rate),\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   3001\u001b[39m         _get_default_replica_context()):\n\u001b[32m   3002\u001b[39m       fn = autograph.tf_convert(\n\u001b[32m   3003\u001b[39m           fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3004\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m self._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3005\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._update(var, fn, args, kwargs, group)\n\u001b[32m   3006\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3007\u001b[39m       return self._replica_ctx_update(\n\u001b[32m   3008\u001b[39m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, var, fn, args, kwargs, group)\u001b[39m\n\u001b[32m   4072\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update(self, var, fn, args, kwargs, group):\n\u001b[32m   4073\u001b[39m     \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4075\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[39m\n\u001b[32m   4077\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\n\u001b[32m   4078\u001b[39m     \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[32m   4079\u001b[39m     \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[32m   4080\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[32m-> \u001b[39m\u001b[32m4081\u001b[39m       result = fn(*args, **kwargs)\n\u001b[32m   4082\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[32m   4083\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   4084\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(var, grad, learning_rate)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m apply_grad_to_update_var(var, grad, learning_rate):\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.update_step(grad, var, learning_rate)\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/optimizers/adam.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, gradient, variable, learning_rate)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m update_step(self, gradient, variable, learning_rate):\n\u001b[32m    103\u001b[39m         \u001b[33m\"\"\"Update step given gradient and the associated model variable.\"\"\"\u001b[39m\n\u001b[32m    104\u001b[39m         lr = ops.cast(learning_rate, variable.dtype)\n\u001b[32m    105\u001b[39m         gradient = ops.cast(gradient, variable.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         local_step = ops.cast(self.iterations + \u001b[32m1\u001b[39m, variable.dtype)\n\u001b[32m    107\u001b[39m         beta_1_power = ops.power(\n\u001b[32m    108\u001b[39m             ops.cast(self.beta_1, variable.dtype), local_step\n\u001b[32m    109\u001b[39m         )\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/ops/core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, dtype)\u001b[39m\n\u001b[32m    831\u001b[39m     >>> x = keras.ops.cast(x, dtype=\u001b[33m\"float16\"\u001b[39m)\n\u001b[32m    832\u001b[39m     \"\"\"\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[32m    834\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Cast(dtype=dtype)(x)\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.core.cast(x, dtype)\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, dtype)\u001b[39m\n\u001b[32m    213\u001b[39m         x = tf.cast(x, dtype)\n\u001b[32m    214\u001b[39m         x.set_shape(x_shape)\n\u001b[32m    215\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.cast(x, dtype=dtype)\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/ops/math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, dtype, name)\u001b[39m\n\u001b[32m   1017\u001b[39m             \u001b[33m\"discard the imaginary part and may not be what you \"\u001b[39m\n\u001b[32m   1018\u001b[39m             \u001b[33m\"intended.\"\u001b[39m\n\u001b[32m   1019\u001b[39m         )\n\u001b[32m   1020\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m x.dtype != base_type:\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m         x = gen_math_ops.cast(x, base_type, name=name)\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[32m~/SCIML/summerschool/.venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, DstT, Truncate, name)\u001b[39m\n\u001b[32m   2114\u001b[39m         _ctx, \u001b[33m\"Cast\"\u001b[39m, name, x, \u001b[33m\"DstT\"\u001b[39m, DstT, \u001b[33m\"Truncate\"\u001b[39m, Truncate)\n\u001b[32m   2115\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   2116\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2117\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m2118\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   2119\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2120\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2121\u001b[39m       return cast_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_history = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56598b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history)\n",
    "#log \n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd8958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
